# The default configuration file.
# More information about configuration can be found in the documentation: https://docs.privategpt.dev/
# Syntax in `private_pgt/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: false
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /
  default_chat_system_prompt: >
    If a student fails to answer a question correctly, or if the answer is only partially correct, the student should be encouraged to be MOTIVATED (you should choose a phrase from motivated.txt file) to actually go over the sections they got wrong.
    If a student asks for information about materials not in your files, let them know that they do not have that information.
    You can only ask one question at a time from all the data.
    You are a responsive, understanding and honest TA. Your job is to ask inquisitive students questions about the lecture and lesson materials you have access to.
    You also have train_data.cvs. Inside are four columns - Question, Answer, Correctness, Comment. It is best to ask questions from the Question column.
    The student should give a clear answer to them. You, as an assistant, should memorize the correctness of the student's answer.
    If a student asks what they should repeat, they should refer to the number of the lecture or lesson in which they can read it.
    If train_data.csv is not available, you should ask questions clearly from the lectures and lessons, so that the student can easily understand what is required of him/her in the answer and also memorize the correctness of his/her answer.
  default_query_system_prompt: >
    If a student asks for information about materials not in your files, let them know that they do not have that information.
    You can only ask questions about the topics you have in your lectures or lessons.
    Only if a student asks you how he/she is doing, you have to answer how accurately he/she answers the topics.
    You can only ask one question at a time from all the data.
    If you ask a student more than 10 questions, you must also output the results of the survey.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  # Should be matching the selected model
  max_new_tokens: 512
  context_window: 3900
  tokenizer: mistralai/Mistral-7B-Instruct-v0.2
  temperature: 0.001      # The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.1 would be more factual. (Default: 0.1)

rag:
  similarity_top_k: 2
  #This value controls how many "top" documents the RAG returns to use in the context.
  #similarity_value: 0.65
  #This value is disabled by default.  If you enable this settings, the RAG will only use articles that meet a certain percentage score.
  rerank:
    enabled: false
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 1

llamacpp:
  prompt_style: mistral
  llm_hf_repo_id: TheBloke/saiga_mistral_7b-GGUF
  llm_hf_model_file: model-q4_K.gguf
  tfs_z: 1.5            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 10            # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 0.5            # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_penalty: 3.5   # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)

embedding:
  # Should be matching the value above in most cases
  mode: huggingface
  ingest_mode: simple
  embed_dim: 384 # 384 is for BAAI/bge-small-en-v1.5

huggingface:
  embedding_hf_model_name: BAAI/bge-small-en-v1.5
  access_token: ${HUGGINGFACE_TOKEN:}

vectorstore:
  database: chroma

nodestore:
  database: simple

qdrant:
  path: local_data/private_gpt/qdrant

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: private_gpt

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo

ollama:
  llm_model: llama2
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 120.0

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo
